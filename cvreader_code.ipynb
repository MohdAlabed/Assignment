{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOS9Hlgkul-O",
        "outputId": "3d344102-b43f-4ac6-e40a-0d3609966d9a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.24.5)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.3 in /usr/local/lib/python3.10/dist-packages (from pymupdf) (1.24.3)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.3/717.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf unidecode fuzzywuzzy\n",
        "# !sudo apt install tesseract-ocr\n",
        "# !pip install pytesseract\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Code"
      ],
      "metadata": {
        "id": "l4F3aBIkSBdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "# import pytesseract\n",
        "# from PIL import Image\n",
        "# from io import BytesIO\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "import re\n",
        "import numpy as np\n",
        "from fuzzywuzzy import process, fuzz\n",
        "from datetime import datetime\n",
        "from dateutil import relativedelta\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GeminiAPI')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')"
      ],
      "metadata": {
        "id": "fZxNNNXKu8Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file = fitz.open('CV.pdf')\n",
        "\n",
        "CV_text = \"\"\n",
        "for page in pdf_file:\n",
        "    CV_text += page.get_text()\n",
        "    if CV_text.strip() == \"\":\n",
        "      imgPDF = page.get_text(\"dict\")\n",
        "      imageVec = [imgPDF['blocks'][0]['image']]\n",
        "      for index, image_bytes in enumerate(imageVec):\n",
        "\n",
        "        image = Image.open(BytesIO(image_bytes))\n",
        "        raw_text = str(pytesseract.image_to_string(image))\n",
        "        CV_text += raw_text\n",
        "\n",
        "    CV_text = CV_text.replace(\"\\n\", \" \")\n",
        "\n",
        "print(CV_text)\n"
      ],
      "metadata": {
        "id": "jrYxPbn8kiTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118d4c6b-293c-4a82-83d0-3340e9964f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mohammed Abdullah mohdalabed@outlook.com   +962796163245   Amman, Jordan, Shafa Badran   Mohammed Abdullah   PROFILE I'm an aspiring fresh graduate from the University of Jordan, Business Information Technology department. I am eager to gain new experiences, take on new challenges, make a positive impact, and be inspired by the individuals I encounter. TRAINING & SPECIALIZATION IBM Data Science Professional Certificate Coursera 03/2024 – present Amman, Jordan Microsoft Services 11/2023 – 01/2024 Amman, Jordan JavaScript and GIS Training GISTEC 09/2022 – 10/2022 Amman, Jordan EDUCATION Bachelor's Degree in Business Information Technology The University of Jordan GPA: 3.38 (Very Good) 10/2019 – 07/2023 Amman, Jordan PROJECTS Advanced Registration Website Project Manager and Team Member | Graduation Project Supervised by: Prof. Ibrahim Aljarah & Prof. Omar Al-Kadi (Project 1) | M. 10/2022 – 06/2023 Mariam Itriq (Project 2) •A full stack implementation with the use of HTML,CSS, BootStrap and JavaScript  for the front-end, ASP.NET/C# for the back-end, Microsoft SQL Server for the  database, and Python for the machine learning. •Worked on a recommendation system for the university’s student registration that  uses machine learning to recommend timed schedules for students which uses  hybrid filtering of collaborative and content-based filtering with the use of NLP  and K-Means Clustering, and a CVXPY optimization function to find the optimal  schedule for each student and apply hard and soft constraints. Students With Special Needs Service System Analyst and Team Member | System Analysis and Design Project Supervised by: Prof. Mohammad A. M. Abushariah 10/2022 – 01/2023 •A system documentation using SDLC plan driven approach and front-end  implementation. •The project was highly appraise by Prof. Mohammad Abushariah as the best  project of that term. Medical System Team Member Supervised by: Prof. Amjad Hudaib 03/2022 – 06/2022 •A software documentation of a medical appointment system using incremental  development method. •One of my key responsibilities was requirements gathering from stack holders by  interviewing doctors and using a questionnaire for patient users. Business Analytics Report Worked on multiple datasets performing different data analysis methods using  Microsoft Excel including descriptive statistic, regression analysis, and predictive  analysis and providing the finding in the form of a business report. 2022 Additionally •Worked on many more project throughout the years, Including: An image filtering  application using MATLAB. An ERP demo using Dolibarr. A Front-end Website  using HTML, CSS, and JavaScript COURSES Tools for Data Science IBM | Coursera 05/2024 What is Data Science? IBM | Coursera 03/2024 Python for Data Science and Machine Learning Udemy 2023 Advanced Facilitation Skills Nahno Center for Training and Development 12/2021 Effective Leadership Techniques Nahno Center for Training and Development 12/2021 Full Stack Development Track Completion Udacity 1 Million Arab Coders Initiative 09/2020 Full Stack Development Track Participation Udacity 1 Million Arab Coders Initiative 06/2020 General English Program United Arab Emirates University 08/2015 VOLUNTEERING A Total Number Of 116 Recorded Volunteering and Participation Opportunities  Hours Through Nahno.org, Most Notably:- •Ambassador For Nahno at Jordan University | 2021/2022 •Translating Articles About Artificial Intelligence On Arabic Wikipedia (32 Hours) – The Arabic Content Club  At The Hashemite University | Oct 2021 •Poetry And Musical Evening (4 Hours) – Maidan Community Service | Oct 2021 •The Open Day With The Children’s Museum (9 Hours) – National Children’s Museum Association | Nov 2021 SOFT SKILLS Hard Working Communication Fast Learner Leadership Adaptability Writing HARD SKILLS Python C++ ASP.NET CSS Execl Java SQL HTML BootStrap Familiar With: PHP • C# • MS SQL • JavaScrip LANGUAGES Arabic Native Speaker English IELTS - 6.0|2019 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Area"
      ],
      "metadata": {
        "id": "qGHgsEf_RxMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PDF = fitz.open('CV.pdf')\n",
        "\n",
        "# Get the dictionary blocks for each page\n",
        "blocks_dict = {}\n",
        "page_num = 1\n",
        "\n",
        "for page in PDF:\n",
        "  file_dictBlocks = page.get_text('dict')['blocks']\n",
        "  blocks_dict[page_num] = file_dictBlocks\n",
        "  page_num += 1\n",
        "\n",
        "# Get the spans, clean, and find if a span is is_upper and is_bold\n",
        "spans = []\n",
        "\n",
        "for page, blocks in blocks_dict.items():\n",
        "  for block in blocks:\n",
        "    if block['type'] == 0:\n",
        "      for line in block['lines']:\n",
        "        for span in line['spans']:\n",
        "\n",
        "          x0, y0, x1, y1 = list(span['bbox'])\n",
        "          font_size = span['size']\n",
        "          text = unidecode(span['text'].strip())\n",
        "          text = re.sub(r'\\s+', ' ', text)\n",
        "          span_font = span['font']\n",
        "          is_upper = False\n",
        "          is_bold = False\n",
        "\n",
        "          if \"bold\" in span_font.lower():\n",
        "            is_bold = True\n",
        "\n",
        "          if text.isupper():\n",
        "            is_upper = True\n",
        "          if text.replace(\" \",\"\") !=  \"\" and font_size > 1:\n",
        "            spans.append((x0, y0, x1, y1, text, is_upper, is_bold, span_font, font_size))\n",
        "\n",
        "# Converting spans to a dataframe\n",
        "span_df = pd.DataFrame(spans, columns=['x0','y0','x1','y1', 'text', 'is_upper','is_bold','span_font', 'font_size'])\n",
        "\n",
        "# Giving a score for each span based in font_size, is_bold, and is_upper\n",
        "span_scores = []\n",
        "\n",
        "for index, span_row in span_df.iterrows():\n",
        "  score = round(span_row.font_size)\n",
        "  text = span_row.text\n",
        "  if span_row.is_bold:\n",
        "    score += 1\n",
        "  if span_row.is_upper:\n",
        "    score += 1\n",
        "\n",
        "  span_scores.append(score)\n",
        "\n",
        "values, freq = np.unique(span_scores, return_counts=True) # Getting unique values and their frequency\n",
        "\n",
        "p_size = max(zip(values, freq), key=lambda pair: pair[1])[0] # Get the key with the highest frequency\n",
        "\n",
        "# Giving each span a tag of hx, sx, or p\n",
        "hx = 0 # Level of headers\n",
        "sx = 0 # Level of smaller text\n",
        "fontSize_tags = {} # Store tags with the value for each tag\n",
        "\n",
        "for value in sorted(values, reverse = True):\n",
        "  if value == p_size:\n",
        "    fontSize_tags[value] = 'p'\n",
        "  if value > p_size:\n",
        "    hx += 1\n",
        "    fontSize_tags[value] = 'h{0}'.format(hx)\n",
        "  if value < p_size:\n",
        "    sx += 1\n",
        "    fontSize_tags[value] = 's{0}'.format(sx)\n",
        "\n",
        "span_tags = [fontSize_tags[score] for score in span_scores] # Giving each span a tag based on scores in span_scores\n",
        "span_df['tag'] = span_tags # Adding column to the span_df dataframe\n",
        "\n",
        "# Structuring the dataframe based on tags\n",
        "PDF_structure = []\n",
        "header_content = \"\" # Store the content of each header\n",
        "i = 0 # Loop over headers\n",
        "j = 0 # Loop over p and s\n",
        "\n",
        "while i < len(span_df): # Looping span_df records\n",
        "  if 'h' in span_df.iloc[i]['tag']: # Finding headers\n",
        "    if i != len(span_df) - 1: # If i is not already on the final row\n",
        "      j = i + 1\n",
        "      while j < len(span_df) and 'h' not in span_df.iloc[j]['tag']: # Getting spans with p and s tags as header content\n",
        "        header_content += (\" \" if header_content else \"\") + span_df.iloc[j]['text']\n",
        "        j += 1\n",
        "\n",
        "    # Storing headers and their content\n",
        "    header = {\n",
        "        span_df.iloc[i]['tag'] : span_df.iloc[i]['text'],\n",
        "        'content' : [header_content]\n",
        "    }\n",
        "    PDF_structure.append(header)\n",
        "    header_content = \"\" # Empty header_content for the next header\n",
        "    if i != len(span_df) - 1: # If i is not already in the final row\n",
        "      i = j # Jump i to the value of j\n",
        "    else:\n",
        "      i += 1\n",
        "  else: # If p or is is not the content of any header\n",
        "    not_header = {\n",
        "        span_df.iloc[i]['tag'] : span_df.iloc[i]['text']\n",
        "    }\n",
        "    PDF_structure.append(not_header)\n",
        "    i += 1\n",
        "\n",
        "# Storing lower header level inside of higher levels\n",
        "headers_tmp = [] # Temporary header store for headers with equal levels under a higher level header\n",
        "\n",
        "for loop in range(hx - 1): # Loop for each header level\n",
        "  i = len(PDF_structure) - 1\n",
        "\n",
        "  while i >= 0:\n",
        "\n",
        "    if next(iter(PDF_structure[i])) > next(iter(PDF_structure[i - 1])): # If the header is a lower level than the previous one\n",
        "      if headers_tmp == []:\n",
        "        PDF_structure[i - 1]['content'].append(PDF_structure[i]) # Store the lower header in the higher level content\n",
        "        del PDF_structure[i] # Delete the last header\n",
        "      else: # If headers_tmp is not empty\n",
        "        headers_tmp.append(PDF_structure[i])\n",
        "        PDF_structure[i - 1]['content'].extend(reversed(headers_tmp)) # Store the content of headers_tmp in the higher level header\n",
        "        del PDF_structure[i] # Delete the last header\n",
        "        headers_tmp = []\n",
        "\n",
        "    elif next(iter(PDF_structure[i])) == next(iter(PDF_structure[i - 1])) and next(iter(PDF_structure[i])) != 'h1': # If headers are of equal levels\n",
        "      headers_tmp.append(PDF_structure[i])\n",
        "      del PDF_structure[i]\n",
        "\n",
        "    else: # If a higher header level is before the a lower level\n",
        "      if headers_tmp != []:\n",
        "        PDF_structure = PDF_structure[:i + 1] + list(reversed(headers_tmp)) + PDF_structure[i + 1:] # Return the content of headers_tmp to their place of no higher level found in this iteration\n",
        "        headers_tmp = []\n",
        "\n",
        "    i -= 1\n",
        "\n",
        "# Common nouns for each header of the required section\n",
        "headers = {\n",
        "    \"work_experience\": ['working experience', 'professional experience', 'employment history', 'experience', 'work experience', 'hands-on experience', 'practical experience', 'work record', 'field experience', 'employment record', 'job experience', 'professional background', 'career history', 'expertise', 'past experience'],\n",
        "    \"skills\": ['skills', 'hard skills', 'technical skills', 'work skills', 'abilities', 'knowledgeable in', 'competencies', 'strengths', 'capabilities'],\n",
        "    \"Education\": ['education', 'education history', 'educational background', 'study', 'academic background', 'university', 'graduation']\n",
        "    }\n",
        "\n",
        "def search_headers(data, target_header):\n",
        "  results = \"\"\n",
        "  found = False\n",
        "\n",
        "  def recursive_search(data, target_header): # Find the target_header\n",
        "    nonlocal found, results\n",
        "\n",
        "    for header_item in data:\n",
        "      if next(iter(header_item.values())) == target_header: # If target_header is found\n",
        "        results += next(iter(header_item.values())) + \"\\n\" + header_item['content'][0]\n",
        "        found = True\n",
        "        if len(header_item['content']) > 1: # If target_header has any content\n",
        "          extract_content(header_item['content'][1:])\n",
        "\n",
        "      elif len(header_item['content']) > 1: # Mine the content of target_header and call it recursively\n",
        "        i = 1\n",
        "        while i < len(header_item['content']) and not found:\n",
        "          recursive_search([header_item['content'][i]], target_header)\n",
        "          i += 1\n",
        "\n",
        "  def extract_content(data): # Exctract the content of target_header\n",
        "    nonlocal results\n",
        "    for inside_item in data:\n",
        "      results += \"\\n\\n\" + next(iter(inside_item.values())) + \"\\n\" + inside_item['content'][0]\n",
        "      if len(inside_item['content']) > 1:\n",
        "        i = 1\n",
        "        while i < len(inside_item['content']):\n",
        "          extract_content([inside_item['content'][i]])\n",
        "          i += 1\n",
        "\n",
        "  recursive_search(data, target_header)\n",
        "  return results\n",
        "\n",
        "# To find the section that has the required data (skills, education, and experience)\n",
        "work_exp = \"\"\n",
        "skills = \"\"\n",
        "education = \"\"\n",
        "\n",
        "for samples in headers.values(): # Iterate over the headers dictionary to find each the required section\n",
        "\n",
        "  header_scores = []\n",
        "  for index, span_row in span_df.iterrows(): # Find the scores for each header store them in tuple\n",
        "    if 'h' in span_row.tag:\n",
        "      scores = [score[1] for score in process.extract(span_row.text, samples, scorer=fuzz.token_sort_ratio)]\n",
        "      avg = sum(scores)/len(scores) # Lenght scores equals 5 by default\n",
        "      header_scores.append((span_row.text, avg)) # Store the average for each header in tuple\n",
        "\n",
        "  word_score = max(header_scores, key=lambda pair: pair[1]) # Get the header with highest score\n",
        "  lookfor = search_headers(PDF_structure, word_score[0]) # Get the header section using the recursive function\n",
        "\n",
        "  # Store each section into a separate variable\n",
        "  if samples == headers['work_experience']:\n",
        "    work_exp = lookfor\n",
        "  elif samples == headers['skills']:\n",
        "    skills = lookfor\n",
        "  else:\n",
        "    education = lookfor\n",
        "\n",
        "# Regular expressions to find the dates\n",
        "date_patterns = [\n",
        "    r'\\d{1,4}\\s?[-,/]?\\s?(?:\\d{1,4})?\\s?[-,]\\s?\\d{1,4}\\s?[-,/]?\\s?(?:\\d{1,4})?',\n",
        "    r'\\d{1,4}\\s?[-,/]?\\s?(?:\\d{1,4})?\\s?[Tt][Oo]\\s?\\d{1,4}\\s?[-,/]?\\s?(?:\\d{1,4})?',\n",
        "    r'\\d{1,4}\\s?[-,/]?\\s?(?:\\d{1,4})?\\s?[-,]\\s?[Pp][Rr][Ee][Ss][Ee][Nn][Tt]'\n",
        "    ]\n",
        "\n",
        "current_year = datetime.now().year\n",
        "current_month = datetime.now().month\n",
        "\n",
        "# Excract the dates that fit the date formats in date_patterns from work_exp\n",
        "dates = []\n",
        "for pattern in date_patterns:\n",
        "    matches = re.findall(pattern, work_exp)\n",
        "    dates.extend(matches)\n",
        "\n",
        "# Calculate total years of experience\n",
        "sum_all = 0\n",
        "sum_years = 0\n",
        "sum_months = 0\n",
        "\n",
        "for date in dates:\n",
        "  date = re.split('\\s?[-,/]\\s?|[Tt][Oo]', date)\n",
        "  twoORone = False\n",
        "  for item in date:\n",
        "    if re.search('[Pp][Rr][Ee][Ss][Ee][Nn][Tt]', str(item)): # Replace the word \"Present\" with current date\n",
        "\n",
        "      if len(str(date[0])) == 2 or len(str(date[0])) == 1: # To figure out the the date format to replace \"Present\"\n",
        "        date[date.index(item)] = current_month\n",
        "        date.append(current_year)\n",
        "      else:\n",
        "        date[date.index(item)] = current_year\n",
        "        date.append(current_month)\n",
        "\n",
        "    else:\n",
        "      date[date.index(item)] = int(item)\n",
        "  if len(date) == 4: # To convert int into date based on the date format\n",
        "    if len(str(date[0])) == 2 or len(str(date[0])) == 1: # mm/yyyy\n",
        "      datetime1 = datetime(date[1], date[0], 1)\n",
        "      datetime2 = datetime(date[3], date[2], 1)\n",
        "\n",
        "    else: # yyyy/mm\n",
        "      print(date[1])\n",
        "      datetime1 = datetime(date[0], date[1], 1)\n",
        "      datetime2 = datetime(date[2], date[3], 1)\n",
        "  else: # yyyy (no month)\n",
        "      datetime1 = datetime(date[0], 1, 1)\n",
        "      datetime2 = datetime(date[1], 1, 1)\n",
        "\n",
        "  if datetime2 > datetime1: # Find the bigger date\n",
        "      twoORone = True\n",
        "\n",
        "  if twoORone: # Find the time_difference between the two dates\n",
        "    time_difference = relativedelta.relativedelta(datetime2, datetime1)\n",
        "  else:\n",
        "    time_difference = relativedelta.relativedelta(datetime1, datetime2)\n",
        "\n",
        "  # Sum the dates years and months only then all together\n",
        "  sum_years += time_difference.years\n",
        "  sum_months += time_difference.months\n",
        "\n",
        "sum_all = sum_years + round(sum_months/12)\n",
        "\n",
        "skills = re.sub(r'\\s+', ' ', re.sub(r'[*]', ' ', re.sub(r'\\n', ' ', skills))) # Clean skills\n"
      ],
      "metadata": {
        "id": "5lBJFL5QnY45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt google gemini to generate questions with the entered position the skills\n",
        "position = input(\"Enter the position you want to generate questions for: \")\n",
        "prompt = model.generate_content(f\"Generate 10 cognitive questions about {position} position, and give it a number from 1 to 3 at the end of the question between () the based on the difficulty of the question (where 3 is more difficult than 1) taking into account the following {skills}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "HTqkgG51XHZ0",
        "outputId": "7571b3eb-9ee0-460a-ef70-9757c031dc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the position you want to generate questions for: Senior Software Developer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and extract the questions and prompt the user to answer them\n",
        "matches = re.findall(r'\\n(\\d+\\..*?\\(\\d+\\))', prompt.text)\n",
        "answers = [] # Store questions and answers\n",
        "print('Please answer the following questions:')\n",
        "for question in matches:\n",
        "  answer = input(question)\n",
        "  answers.append((question, answer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac7zctw-YwK4",
        "outputId": "583ae45c-3754-42a8-9b38-91ffbcb6ce40"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please answer the following questions:\n",
            "1. **Describe a situation where you had to debug a complex software issue in a large codebase. How did you approach the problem, and what tools or techniques did you employ? (2)In a previous role, I encountered a complex software issue in a large codebase where a critical feature was intermittently failing. I approached the problem by first gathering detailed logs and error messages to pinpoint when and where the issue occurred. Using debugging tools like breakpoints in the IDE, I traced the execution flow and examined variable values to identify anomalies. Collaborating with team members, we reviewed the code together, conducting peer reviews and brainstorming sessions to explore potential causes and solutions. Ultimately, isolating the root cause required meticulous attention to detail and systematic testing of hypotheses until we successfully resolved the issue.\n",
            "2. **Explain the trade-offs involved in choosing between a relational database (MS SQL) and a NoSQL database for a specific application. When would you choose one over the other? (2)Choosing between MS SQL (a relational database) and a NoSQL database involves:  Relational Database (MS SQL):  Strengths: Strong consistency, ACID transactions, structured data. Trade-offs: Less flexible schema, vertical scalability. NoSQL Database:  Strengths: Flexible schema, horizontal scalability, high performance. Trade-offs: Eventual consistency, may lack ACID transactions for all operations. Choose MS SQL for structured data with complex relationships and transactions. Choose NoSQL for scalability, flexibility with semi-structured or unstructured data, and high performance.\n",
            "3. **How would you architect a RESTful API using ASP.NET Web API to handle a large number of concurrent requests? What considerations would you take into account for performance and scalability? (3)To architect a RESTful API in ASP.NET Web API for handling a large number of concurrent requests:  Use Asynchronous Programming: Utilize async/await to handle I/O-bound operations efficiently without blocking threads.  Implement Caching: Employ caching mechanisms (e.g., in-memory caching, distributed caching with Redis) to reduce database load and improve response times.  Optimize Database Access: Use connection pooling, optimize queries, and consider database sharding or partitioning for scalability.  Scale Horizontally: Deploy multiple API instances behind a load balancer to distribute traffic and handle more concurrent requests.  Monitor and Tune: Continuously monitor performance metrics, identify bottlenecks, and fine-tune configurations (e.g., thread pool settings, request timeouts) accordingly.  These measures help ensure performance and scalability under high loads in a RESTful API built with ASP.NET Web API.\n",
            "4. **You are tasked with optimizing the performance of a Python web application. What are some common bottlenecks you would investigate, and how would you approach identifying and addressing them? (2)Common bottlenecks in Python web applications often include:  Database Queries: Optimize queries, use indexes, and consider caching strategies. CPU Bound Tasks: Profile code, optimize algorithms, and consider parallelism or async. I/O Operations: Use async I/O, caching, and minimize blocking calls. Memory Leaks: Monitor memory usage, identify leaks with profiling tools, and optimize data structures. To address these, I'd start with profiling tools like cProfile or line_profiler, optimize critical paths, and implement caching and async techniques where appropriate.\n",
            "5. **Explain the difference between inheritance and composition in object-oriented programming. Provide examples of when each is a better choice. (1)nheritance allows one class to inherit properties and methods from another, promoting code reuse and creating an \"is-a\" relationship (e.g., a Car is a Vehicle). Composition involves creating objects within another class to achieve more complex behaviors, forming a \"has-a\" relationship (e.g., a Car has an Engine). Use inheritance when there is a clear hierarchical relationship and you want to reuse code; use composition when you need flexibility and want to create complex objects from simpler ones.\n",
            "6. **Describe how you would use CSS preprocessors like SASS or LESS to improve the maintainability and scalability of a complex web application's stylesheets. (2)CSS preprocessors like SASS or LESS improve maintainability and scalability of web application stylesheets by enabling variables for consistent values, mixins for reusable styles, nesting for hierarchical structure, and functions for complex calculations. This results in cleaner, more organized code that is easier to maintain and extend as the application grows.\n",
            "7. **You are working on a project where the frontend team is using React and the backend team is using Spring Boot. Explain how you would approach integrating the two technologies, considering data transfer and communication protocols. (3)To integrate React frontend with Spring Boot backend:  Use RESTful APIs for data transfer. Define clear JSON data structures for communication. Ensure CORS is configured correctly. Implement authentication and authorization mechanisms. Use Axios or Fetch API for making HTTP requests from React to Spring Boot.\n",
            "8. **How would you use unit testing and integration testing to ensure the quality and reliability of your code during development? (1)Unit testing ensures that individual units of code (functions, methods) work correctly in isolation, while integration testing verifies that these units work together as expected in the entire system. By writing and running unit tests frequently during development and integration tests before deployment, we ensure code quality and reliability at both micro and macro levels of the application.\n",
            "9. **Explain the concept of SOLID principles in object-oriented programming. How do they contribute to building maintainable and extensible code? (2)SOLID principles in object-oriented programming are guidelines to help developers create more maintainable and extensible software:  Single Responsibility Principle (SRP): Each class should have only one reason to change. Open/Closed Principle (OCP): Software entities should be open for extension but closed for modification. Liskov Substitution Principle (LSP): Subtypes must be substitutable for their base types without altering correctness. Interface Segregation Principle (ISP): Clients should not be forced to depend on interfaces they do not use. Dependency Inversion Principle (DIP): Depend on abstractions, not concrete implementations. Following these principles reduces code fragility, improves code readability, and facilitates easier maintenance and scalability over time.\n",
            "10. **Describe your experience with agile development methodologies like Scrum or Kanban. How have you applied these methodologies to improve project collaboration and efficiency? (1)In my experience with agile methodologies like Scrum and Kanban, I've seen significant improvements in project collaboration and efficiency. By implementing daily stand-ups, sprint planning, and iterative feedback loops, teams can adapt quickly to changing requirements and deliver high-quality results on time.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1. **Describe a situation where you had to debug a complex software issue in a large codebase. How did you approach the problem, and what tools or techniques did you employ? (2)',\n",
              "  'In a previous role, I encountered a complex software issue in a large codebase where a critical feature was intermittently failing. I approached the problem by first gathering detailed logs and error messages to pinpoint when and where the issue occurred. Using debugging tools like breakpoints in the IDE, I traced the execution flow and examined variable values to identify anomalies. Collaborating with team members, we reviewed the code together, conducting peer reviews and brainstorming sessions to explore potential causes and solutions. Ultimately, isolating the root cause required meticulous attention to detail and systematic testing of hypotheses until we successfully resolved the issue.'),\n",
              " ('2. **Explain the trade-offs involved in choosing between a relational database (MS SQL) and a NoSQL database for a specific application. When would you choose one over the other? (2)',\n",
              "  'Choosing between MS SQL (a relational database) and a NoSQL database involves:  Relational Database (MS SQL):  Strengths: Strong consistency, ACID transactions, structured data. Trade-offs: Less flexible schema, vertical scalability. NoSQL Database:  Strengths: Flexible schema, horizontal scalability, high performance. Trade-offs: Eventual consistency, may lack ACID transactions for all operations. Choose MS SQL for structured data with complex relationships and transactions. Choose NoSQL for scalability, flexibility with semi-structured or unstructured data, and high performance.'),\n",
              " ('3. **How would you architect a RESTful API using ASP.NET Web API to handle a large number of concurrent requests? What considerations would you take into account for performance and scalability? (3)',\n",
              "  'To architect a RESTful API in ASP.NET Web API for handling a large number of concurrent requests:  Use Asynchronous Programming: Utilize async/await to handle I/O-bound operations efficiently without blocking threads.  Implement Caching: Employ caching mechanisms (e.g., in-memory caching, distributed caching with Redis) to reduce database load and improve response times.  Optimize Database Access: Use connection pooling, optimize queries, and consider database sharding or partitioning for scalability.  Scale Horizontally: Deploy multiple API instances behind a load balancer to distribute traffic and handle more concurrent requests.  Monitor and Tune: Continuously monitor performance metrics, identify bottlenecks, and fine-tune configurations (e.g., thread pool settings, request timeouts) accordingly.  These measures help ensure performance and scalability under high loads in a RESTful API built with ASP.NET Web API.'),\n",
              " ('4. **You are tasked with optimizing the performance of a Python web application. What are some common bottlenecks you would investigate, and how would you approach identifying and addressing them? (2)',\n",
              "  \"Common bottlenecks in Python web applications often include:  Database Queries: Optimize queries, use indexes, and consider caching strategies. CPU Bound Tasks: Profile code, optimize algorithms, and consider parallelism or async. I/O Operations: Use async I/O, caching, and minimize blocking calls. Memory Leaks: Monitor memory usage, identify leaks with profiling tools, and optimize data structures. To address these, I'd start with profiling tools like cProfile or line_profiler, optimize critical paths, and implement caching and async techniques where appropriate.\"),\n",
              " ('5. **Explain the difference between inheritance and composition in object-oriented programming. Provide examples of when each is a better choice. (1)',\n",
              "  'nheritance allows one class to inherit properties and methods from another, promoting code reuse and creating an \"is-a\" relationship (e.g., a Car is a Vehicle). Composition involves creating objects within another class to achieve more complex behaviors, forming a \"has-a\" relationship (e.g., a Car has an Engine). Use inheritance when there is a clear hierarchical relationship and you want to reuse code; use composition when you need flexibility and want to create complex objects from simpler ones.'),\n",
              " (\"6. **Describe how you would use CSS preprocessors like SASS or LESS to improve the maintainability and scalability of a complex web application's stylesheets. (2)\",\n",
              "  'CSS preprocessors like SASS or LESS improve maintainability and scalability of web application stylesheets by enabling variables for consistent values, mixins for reusable styles, nesting for hierarchical structure, and functions for complex calculations. This results in cleaner, more organized code that is easier to maintain and extend as the application grows.'),\n",
              " ('7. **You are working on a project where the frontend team is using React and the backend team is using Spring Boot. Explain how you would approach integrating the two technologies, considering data transfer and communication protocols. (3)',\n",
              "  'To integrate React frontend with Spring Boot backend:  Use RESTful APIs for data transfer. Define clear JSON data structures for communication. Ensure CORS is configured correctly. Implement authentication and authorization mechanisms. Use Axios or Fetch API for making HTTP requests from React to Spring Boot.'),\n",
              " ('8. **How would you use unit testing and integration testing to ensure the quality and reliability of your code during development? (1)',\n",
              "  'Unit testing ensures that individual units of code (functions, methods) work correctly in isolation, while integration testing verifies that these units work together as expected in the entire system. By writing and running unit tests frequently during development and integration tests before deployment, we ensure code quality and reliability at both micro and macro levels of the application.'),\n",
              " ('9. **Explain the concept of SOLID principles in object-oriented programming. How do they contribute to building maintainable and extensible code? (2)',\n",
              "  'SOLID principles in object-oriented programming are guidelines to help developers create more maintainable and extensible software:  Single Responsibility Principle (SRP): Each class should have only one reason to change. Open/Closed Principle (OCP): Software entities should be open for extension but closed for modification. Liskov Substitution Principle (LSP): Subtypes must be substitutable for their base types without altering correctness. Interface Segregation Principle (ISP): Clients should not be forced to depend on interfaces they do not use. Dependency Inversion Principle (DIP): Depend on abstractions, not concrete implementations. Following these principles reduces code fragility, improves code readability, and facilitates easier maintenance and scalability over time.'),\n",
              " ('10. **Describe your experience with agile development methodologies like Scrum or Kanban. How have you applied these methodologies to improve project collaboration and efficiency? (1)',\n",
              "  \"In my experience with agile methodologies like Scrum and Kanban, I've seen significant improvements in project collaboration and efficiency. By implementing daily stand-ups, sprint planning, and iterative feedback loops, teams can adapt quickly to changing requirements and deliver high-quality results on time.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt google gemini to grade the answers\n",
        "prompt = model.generate_content(f\"Provide ONLY the total grade out of 100 and the grades for the following question, answer pairs. Take into account the numbers (1) to (3) the higher the number the higher the grade that make up the total 100 grade, and keep in mind the the questions with the same number (1), (2), (3) must be marked out of the same grade: {answers}\")"
      ],
      "metadata": {
        "id": "IKcZIVQCQDAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a txt file to write the results on it\n",
        "with open('CV_Report.txt', 'w') as f:\n",
        "    f.write(\"- \" + work_exp + '\\n\\n')\n",
        "    f.write(\"- \" + skills + '\\n\\n')\n",
        "    f.write(\"- \" + education + '\\n\\n')\n",
        "    f.write(f\"- The candadit has: {sum_all} years of experience and is applaying for the {position} position\\n\\n\")\n",
        "    f.write(f\"- Candadit's questions and answers:\\n\\n\")\n",
        "    for q,a in answers:\n",
        "      f.write(f\"Q:{q} \\n A:{a}\\n\")\n",
        "    f.write(f\"\\n\\n- Candadit's grades:\\n {prompt.text}\")"
      ],
      "metadata": {
        "id": "CJZVnEOQzj1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "while len(PDF_structure) != c:\n",
        "  print(\"-------------------------------------\\n\", PDF_structure[i], \" i = \", i)\n",
        "  if next(iter(PDF_structure[i])) > next(iter(PDF_structure[i - 1])):\n",
        "    if headers_tmp == []:\n",
        "      print(\"header_tmp is empty\", PDF_structure[i - 1], PDF_structure[i])\n",
        "      PDF_structure[i - 1]['content'].append(PDF_structure[i])\n",
        "      del PDF_structure[i]\n",
        "    else:\n",
        "      headers_tmp.append(PDF_structure[i])\n",
        "      print(\"header_tmp:\", headers_tmp, 'PDF_structure[i - 1]' ,PDF_structure[i - 1], PDF_structure[i])\n",
        "      PDF_structure[i - 1]['content'].extend(reversed(headers_tmp))\n",
        "      del PDF_structure[i]\n",
        "      headers_tmp = []\n",
        "    i -= 1\n",
        "\n",
        "  elif next(iter(PDF_structure[i])) == next(iter(PDF_structure[i - 1])) and next(iter(PDF_structure[i])) != 'h1':\n",
        "    headers_tmp.append(PDF_structure[i])\n",
        "    print(headers_tmp, \"equal\", PDF_structure[i - 1], PDF_structure[i])\n",
        "    del PDF_structure[i]\n",
        "    i -= 1\n",
        "\n",
        "  else:\n",
        "    if headers_tmp != [] and next(iter(PDF_structure[i])) != 'h1':\n",
        "      j = len(PDF_structure) - c - 1\n",
        "      while headers_tmp != []:\n",
        "        if next(iter(headers_tmp[0])) > next(iter(PDF_structure[j])):\n",
        "          PDF_structure[j]['content'].extend(reversed(headers_tmp))\n",
        "          headers_tmp = []\n",
        "        elif j < 0:\n",
        "          # print(PDF_structure[i],'...', headers_tmp) #For testing purposes\n",
        "          PDF_structure = PDF_structure[:i + 1] + reversed(headers_tmp) + PDF_structure[i + 1:]\n",
        "          c += len(headers_tmp)\n",
        "          headers_tmp = []\n",
        "          i = len(PDF_structure) - c - 1\n",
        "        else:\n",
        "          j -= 1\n",
        "    else:\n",
        "      j = i - 1\n",
        "      higher_H = False\n",
        "\n",
        "      if next(iter(PDF_structure[i])) != 'h1':\n",
        "        while j >= 0:\n",
        "          if next(iter(PDF_structure[i])) > next(iter(PDF_structure[j])):\n",
        "            higher_H = True\n",
        "            break\n",
        "          else:\n",
        "            j -= 1\n",
        "        if higher_H == False:\n",
        "          c += 1\n",
        "          i = len(PDF_structure) - c - 1\n",
        "        else:\n",
        "          i -=1\n",
        "\n",
        "      else:\n",
        "        c += 1\n",
        "        i = len(PDF_structure) - c - 1\n",
        "  print(c,'\\n',PDF_structure)'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-kbU3VIig7v",
        "outputId": "0b27a95b-cce0-423a-fd48-3e9e2ae4a9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'h1': 'Mohammed Abdullah',\n",
              " 'content': ['mohdalabed@outlook.com +962796163245 Amman, Jordan, Shafa Badran Mohammed Abdullah']}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}